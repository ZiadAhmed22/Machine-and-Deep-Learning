{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["#Importing required libraries\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","import os\n","from tensorflow.keras.utils import to_categorical\n","from glob import glob\n","import numpy as np \n","import pandas as pd \n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import auc,roc_curve"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df =pd.read_csv( \"../input/meta-df/meta (1).csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df['age'].fillna(int(df['age'].mean()),inplace=True)\n","df['sex'].fillna((df['sex'].mode()[0]),inplace=True)\n","df['anatom_site_general'].fillna((df['anatom_site_general'].mode()[0]),inplace=True)\n","df['lesion_id'].fillna((df['lesion_id'].mode()[0]),inplace=True)\n","df=df.dropna(subset=['dx'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["base_skin_dir = os.path.join('..',' ../input/skin-ds/dataset')\n","\n","# Merging images from both folders HAM10000_images_part1.zip and HAM10000_images_part2.zip into one dictionary\n","\n","imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n","                     for x in glob(os.path.join(base_skin_dir,  '*.jpg'))}\n","\n","# This dictionary is useful for displaying more human-friendly labels later on\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df['dx'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["lesion_classes_dict ={\n","'mel':'Melanoma',\n","'bkl':'Benign keratosis-like lesions',\n","'bcc':'Basal cell carcinoma',\n","'ak':'Actinic keratoses',\n","'vasc':'Vascular lesions',\n","'df':'Dermatofibroma',\n","'scc':'Squamous cell carcinoma '    \n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["base_skin_dir = '../input/skin-ds/dataset'\n","\n","# Merge images from both folders into one dictionary\n","\n","imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n","                     for x in glob(os.path.join(base_skin_dir,'*.jpg'))}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df['path'] = df['image_id'].map(imageid_path_dict.get)\n","df['cell_type'] = df['dx'].map(lesion_classes_dict.get) \n","df['cell_type_idx'] = pd.Categorical(df['cell_type']).codes\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df['image'] = df['path'].map(lambda x: np.asarray(Image.open(x).resize((125,100))))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df= df[df['age'] != 0]\n","df= df[df['sex'] != 'unknown']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","import tensorflow as tf\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["features=df.drop(columns=['cell_type_idx'],axis=1)\n","target=df['cell_type_idx']"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(features, target, test_size=0.25,random_state=666)\n","tf.unique(x_train_o.cell_type.values)\n","#x_train_o =tf.convert_to_tensor(x_train_o, dtype=tf.float32)\n","#tensor = tf.convert_to_tensor(x_train_o.cell_type.values)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_train = np.asarray(x_train_o['image'].tolist())\n","x_test = np.asarray(x_test_o['image'].tolist())\n","\n","x_train_mean = np.mean(x_train)\n","x_train_std = np.std(x_train)\n","\n","\n","x_test_mean = np.mean(x_test)\n","x_test_std = np.std(x_test)\n","\n","x_train = (x_train - x_train_mean)/x_train_std\n","x_test = (x_test - x_test_mean)/x_test_std"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["y_train = to_categorical(y_train_o, num_classes = 7)\n","y_test = to_categorical(y_test_o, num_classes = 7)\n","y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.1, random_state = 999)\n","# Reshape image in 3 dimensions (height = 100, width = 125 , canal = 3)\n","x_train = x_train.reshape(x_train.shape[0], *(100, 125, 3))\n","x_test = x_test.reshape(x_test.shape[0], *(100, 125, 3))\n","x_validate = x_validate.reshape(x_validate.shape[0], *(100, 125, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_train = x_train.reshape(8400,125*100*3)\n","x_test = x_test.reshape(3112,125*100*3)\n","print(x_train.shape)\n","print(x_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization,Conv2D, MaxPool2D\n","from tensorflow import keras\n","from keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization,Conv2D,MaxPooling2D\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.metrics import Recall\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n","from sklearn.metrics import classification_report,confusion_matrix\n","import cv2\n","from tqdm import tqdm\n","from keras import regularizers\n","import tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["LR = ReduceLROnPlateau(monitor='val_accuracy', \n","                                            patience=4, \n","                                            verbose=1, \n","                                            factor=0.5, \n","                                            min_lr=0.00001)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.layers import BatchNormalization\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.convolutional import AveragePooling2D\n","from keras.layers.convolutional import MaxPooling2D\n","from keras.layers.core import Activation\n","from keras.layers.core import Dropout\n","from keras.layers.core import Dense\n","from keras.layers import Flatten\n","from keras.layers import Input\n","from keras.models import Model\n","from keras.layers import concatenate\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","%matplotlib inline\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import LearningRateScheduler\n","from tensorflow.keras.optimizers import SGD\n","from keras.datasets import cifar10\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def conv_module(input,No_of_filters,filtersizeX,filtersizeY,stride,chanDim,padding=\"same\"):\n","  input = Conv2D(No_of_filters,(filtersizeX,filtersizeY),strides=stride,padding=padding)(input)\n","  input = BatchNormalization(axis=chanDim)(input)\n","  input = Activation(\"relu\")(input)\n","  return input"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def inception_module(input,numK1x1,numK3x3,numk5x5,numPoolProj,chanDim):\n","                                 #Step 1\n","  conv_1x1 = conv_module(input, numK1x1, 1, 1,(1, 1), chanDim) \n","                                 #Step 2\n","  conv_3x3 = conv_module(input, numK3x3, 3, 3,(1, 1), chanDim)\n","  conv_5x5 = conv_module(input, numk5x5, 5, 5,(1, 1), chanDim)\n","                                 #Step 3\n","  pool_proj = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(input)\n","  pool_proj = Conv2D(numPoolProj, (1, 1), padding='same', activation='relu')(pool_proj)\n","                                 #Step 4\n","  input = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=chanDim)\n","  return input"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def downsample_module(input,No_of_filters,chanDim):\n","  conv_3x3=conv_module(input,No_of_filters,3,3,(2,2),chanDim,padding=\"valid\")\n","  pool = MaxPooling2D((3,3),strides=(2,2))(input)\n","  input = concatenate([conv_3x3,pool],axis=chanDim)\n","  return input"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def MiniGoogleNet(width,height,depth,classes):\n","  inputShape=(height,width,depth)\n","  chanDim=-1\n","\n","  # (Step 1) Define the model input\n","  inputs = Input(shape=inputShape)\n","\n","  # First CONV module\n","  x = conv_module(inputs, 96, 3, 3, (1, 1),chanDim)\n","\n","  # (Step 2) Two Inception modules followed by a downsample module\n","  x = inception_module(x, 32, 32,32,32,chanDim)\n","  x = inception_module(x, 32, 48, 48,32,chanDim)\n","  x = downsample_module(x, 80, chanDim)\n","  \n","  # (Step 3) Five Inception modules followed by a downsample module\n","  x = inception_module(x, 112, 48, 32, 48,chanDim)\n","  x = inception_module(x, 96, 64, 32,32,chanDim)\n","  x = inception_module(x, 80, 80, 32,32,chanDim)\n","  x = inception_module(x, 48, 96, 32,32,chanDim)\n","  x = inception_module(x, 112, 48, 32, 48,chanDim)\n","  x = downsample_module(x, 96, chanDim)\n","\n","  # (Step 4) Two Inception modules followed\n","  x = inception_module(x, 176, 160,96,96, chanDim)\n","  x = inception_module(x, 176, 160, 96,96,chanDim)\n","  \n","  # Global POOL and dropout\n","  x = AveragePooling2D((7, 7))(x)\n","  x = Dropout(0.5)(x)\n","\n","  # (Step 5) Softmax classifier\n","  x = Flatten()(x)\n","  x = Dense(classes)(x)\n","  x = Activation(\"softmax\")(x)\n","\n","  # Create the model\n","  model = Model(inputs, x, name=\"googlenet\")\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.1, random_state = 999)\n","# Reshape image in 3 dimensions (height = 100, width = 125 , canal = 3)\n","x_train = x_train.reshape(x_train.shape[0], *(100, 125, 3))\n","x_test = x_test.reshape(x_test.shape[0], *(100, 125, 3))\n","x_validate = x_validate.reshape(x_validate.shape[0], *(100, 125, 3))\n","# With data augmentation to prevent overfitting \n","\n","datagen = ImageDataGenerator(\n","        featurewise_center=False,  # set input mean to 0 over the dataset\n","        samplewise_center=False,  # set each sample mean to 0\n","        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","        samplewise_std_normalization=False,  # divide each input by its std\n","        zca_whitening=False,  # apply ZCA whitening\n","        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n","        zoom_range = 0.1, # Randomly zoom image \n","        width_shift_range=0.12,  # randomly shift images horizontally (fraction of total width)\n","        height_shift_range=0.12,  # randomly shift images vertically (fraction of total height)\n","        horizontal_flip=True,  # randomly flip images\n","        vertical_flip=True)  # randomly flip images\n","\n","datagen.fit(x_train)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["callbacks=[LearningRateScheduler(poly_decay)]\n","opt = SGD(lr=INIT_LR, momentum=0.9)\n","model = MiniGoogleNet(width=125, height=100, depth=3, classes=7)\n","                                    # Step 1\n","model.compile(loss=\"categorical_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n","                                    # Step 2\n","model.fit(datagen.flow(x_train, y_train, batch_size=64),validation_data=(x_validate, y_validate), steps_per_epoch=len(x_train) // 64,epochs=50, callbacks=[LR], verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
