{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import sys\n",
    "import math\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from glob import iglob\n",
    "from PIL import Image , ImageDraw\n",
    "from glob import glob\n",
    "from PIL import Image as pil_image\n",
    "from matplotlib.pyplot import imshow, imsave\n",
    "from IPython.display import Image as Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from glob import glob\n",
    "from PIL import Image as pil_image\n",
    "from matplotlib.pyplot import imshow, imsave\n",
    "from IPython.display import Image as Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "#from keras.optimizers import Adam, SGD\n",
    "from keras import regularizers, initializers\n",
    "from keras.layers.advanced_activations import LeakyReLU, ReLU, Softmax\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import *\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('meta.csv')\n",
    "main_df.head()\n",
    "\n",
    "#We have some missing values in the \"age\" feature. So, lets fill it either use mean or median of the \"age\" feature\n",
    "print(main_df['age'].mean())\n",
    "print(main_df['age'].median())\n",
    "main_df['age'].fillna(main_df['age'].mean(),inplace=True)\n",
    "#Most of the affected people are their ages \n",
    "sns.kdeplot(main_df['age'],shade=True)\n",
    "#Lets create a dictionary to feed in label values in our dataframe\n",
    "lesion_type_dict = {'nv': 'Melanocytic nevi','mel': 'Melanoma','bkl': 'Benign keratosis-like lesions ','bcc': 'Basal cell carcinoma','akiec': 'Actinic keratoses','vasc': 'Vascular lesions','df': 'Dermatofibroma'\n",
    "}\n",
    "lesion_classes_dict ={\n",
    "0:'nv',\n",
    "1:'mel',\n",
    "2:'bkl',\n",
    "3:'bcc',\n",
    "4:'akiec',\n",
    "5:'vasc',\n",
    "6:'df'\n",
    "}\n",
    "main_df['cell_type'] = main_df['dx'].map(lesion_type_dict)\n",
    "main_df['cell_type_idx'] = pd.Categorical(main_df['cell_type']).codes\n",
    "sns.catplot(x=\"sex\", y=\"cell_type_idx\",\n",
    "                    hue=\"sex\",\n",
    "                    data=main_df,\n",
    "                    kind=\"violin\")\n",
    "image_path = {os.path.splitext(os.path.basename(x))[0]: x for x in glob(os.path.join('', '*', '*.jpg'))}\n",
    "main_df['path'] = main_df['image_id'].map(image_path.get)\n",
    "main_df.head()\n",
    "image_example = np.asarray(pil_image.open(main_df['path'][0]))\n",
    "image_example.shape\n",
    "plt.imshow(image_example)\n",
    "main_df['image'] = main_df['path'].map(lambda x: np.asarray(pil_image.open(x).resize((120,90))))\n",
    "main_df.head()\n",
    "plt.imshow(main_df['image'][0])\n",
    "main_df['image'][0].shape\n",
    "fig,axes = plt.subplots(7,5,figsize=(20,21))\n",
    "for nth_axis,(cell_type_name,cell_type_row) in zip(axes,main_df.sort_values(['cell_type']).groupby('cell_type')):\n",
    "    nth_axis[0].set_title(cell_type_name)\n",
    "    for column_axis,(_,column_row) in zip(nth_axis,cell_type_row.sample(5).iterrows()):\n",
    "        column_axis.imshow(column_row['image'])  \n",
    "        column_axis.axis('off')\n",
    "        #Let's split the dataset into Trainig and Validation set.\n",
    "        features = main_df.drop(['cell_type_idx'],axis=1)\n",
    "\n",
    "target = main_df['cell_type_idx']\n",
    "\n",
    "X_TRAIN, X_TEST, Y_TRAIN, Y_TEST = train_test_split(features,target,test_size=0.01)\n",
    "#Creating Image Training and Test set\n",
    "x_train = np.asarray(X_TRAIN['image'].tolist())\n",
    "x_test = np.asarray(X_TEST['image'].tolist())\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "train_mean = x_train.mean()\n",
    "train_std = x_train.std()\n",
    "test_mean = x_test.mean()\n",
    "test_std = x_test.std()\n",
    "\n",
    "print(train_mean)\n",
    "print(train_std)\n",
    "print(test_mean)\n",
    "print(test_std)\n",
    "\n",
    "#calculate mean and Standard deviation of the image array's and then Standardize the image pixel values using the following formula:\n",
    "_train = (x_train-train_mean) / train_std\n",
    "x_test = (x_test-test_mean) / test_std\n",
    "\n",
    "print(Y_TRAIN.shape)\n",
    "print(Y_TEST.shape)\n",
    "\n",
    "y_train = to_categorical(Y_TRAIN,num_classes=7)\n",
    "y_test = to_categorical(Y_TEST,num_classes=7)\n",
    "print(y_train.shape)\n",
    "X_train,X_val, Y_train,Y_val = train_test_split(x_train,y_train,test_size=0.15)\n",
    "X_train  = X_train.reshape(X_train.shape[0],90,120,3)\n",
    "x_test  = x_test.reshape(x_test.shape[0],90,120,3)\n",
    "X_val  = X_val.reshape(X_val.shape[0],90,120,3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(x_test.shape)\n",
    "print(X_val.shape)\n",
    "input_shape = (90, 120, 3)\n",
    "num_classes = 7\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=(3, 3),activation='relu',name=\"conv1\", input_shape=input_shape)) #88,118\n",
    "model.add(BatchNormalization(name=\"Norm1\"))\n",
    "model.add(Conv2D(64,kernel_size=(3, 3), activation='relu',name=\"Conv2\")) #86,116\n",
    "model.add(BatchNormalization(name=\"Norm2\"))\n",
    "model.add(Conv2D(64,kernel_size=(3, 3), activation='relu',name=\"Conv3\")) #84,114\n",
    "model.add(BatchNormalization(name=\"Norm3\"))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2))) # 42,57\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu',name=\"conv4\")) #40,55\n",
    "model.add(BatchNormalization(name=\"Norm4\"))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',name=\"conv5\")) #38,53\n",
    "model.add(BatchNormalization(name=\"Norm5\"))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',name=\"conv6\")) #36,51\n",
    "model.add(BatchNormalization(name=\"Norm6\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #18,25\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',name=\"conv7\")) #16,23\n",
    "model.add(BatchNormalization(name=\"Norm7\"))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu',name=\"conv8\")) #14,21\n",
    "model.add(BatchNormalization(name=\"Norm8\"))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu',name=\"conv9\")) #12,19\n",
    "model.add(BatchNormalization(name=\"Norm9\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #6,9\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv2D(7,(1,1),name=\"conv10\",activation=\"relu\")) #6,9\n",
    "model.add(BatchNormalization(name=\"Norm10\"))\n",
    "model.add(Conv2D(7,kernel_size=(6,9),name=\"conv11\"))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=4, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.0001, \n",
    "                                            min_lr=0.000001)\n",
    "\n",
    "plot_model(model, to_file='simple_model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "Image(retina=True, filename='simple_model_plot.png')\n",
    "# With data augmentation to prevent overfitting \n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1 # Randomly zoom image \n",
    ")\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 1, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])\n",
    "\n",
    "def plot_(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    f, [ax1, ax2] = plt.subplots(1,2, figsize=(15, 5))\n",
    "    ax1.plot(range(len(acc)), acc, label=\"acc\")\n",
    "    ax1.plot(range(len(acc)), val_acc, label=\"val_acc\")\n",
    "    ax1.set_title(\"Training Accuracy vs Validation Accuracy\")\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(range(len(loss)), loss, label=\"loss\")\n",
    "    ax2.plot(range(len(loss)), val_loss, label=\"val_loss\")\n",
    "    ax2.set_title(\"Training Loss vs Validation Loss\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    \n",
    "plot_(model.history)\n",
    "print(\"MAXIMUM ACCURACY OF SIMPLE SEQUENTIAL NETWORK is : \", round(max(model.history.history['val_acc'])*100,4))\n",
    "## Let's randomly download a image from internet to test the model.\n",
    "import urllib\n",
    "\n",
    "url=\"https://lazaderm.com/media/library/fluid-ext-editor-widget/124/image/LZ_VascularLesions%20(2)-crop-1244x1009-40932.jpg\"\n",
    "\n",
    "urllib.request.urlretrieve(url,'test.jpg')\n",
    "test_image = np.asarray(pil_image.open('test.jpg'))\n",
    "print('Original Shape of image is : ',test_image.shape)\n",
    "plt.imshow(test_image)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f7c4d44365b28014734406e4d617c1e1f76ea196def854c7b951a230f6e24f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
